#!/usr/bin/env python3
"""
Progressive Validation Test: Step 3 - Full Scale Espoo Validation

Tests all available Espoo listings with comprehensive performance benchmarks.
This implements full-scale production validation with advanced analytics.

Success Criteria: â‰¥99.40% match rate for production readiness
Final Step: Production deployment validation

Requirements: 5.1, 5.2, 5.3, 5.4
"""

import sys
import unittest
from pathlib import Path
import pandas as pd
import geopandas as gpd
from shapely.geometry import Point
import duckdb
import random
from datetime import datetime, timedelta
import json
import time
import warnings
from concurrent.futures import ThreadPoolExecutor, as_completed
import multiprocessing as mp
import psutil
import gc

# Add the project root to path for package imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# Import from package structure
try:
    from oikotie.database.models import DatabaseManager
    from oikotie.utils import extract_postal_code
except ImportError as e:
    print(f"âŒ Package import failed: {e}")
    print("ğŸ’¡ Ensure package structure is properly initialized")
    sys.exit(1)


class TestEspooStep3FullScale(unittest.TestCase):
    """Test class for full-scale Espoo validation with production benchmarks"""
    
    def setUp(self):
        """Set up test environment"""
        self.db_path = "data/real_estate.duckdb"
        self.osm_buildings_path = "data/helsinki_buildings_20250711_041142.geojson"
        self.output_dir = Path("output/validation/espoo/")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Test parameters
        self.required_match_rate = 99.40  # Production standard
        self.city = "Espoo"
        self.max_workers = min(8, mp.cpu_count())
        self.chunk_size = 500  # Process in chunks for memory efficiency
        
        # Performance thresholds
        self.max_processing_time_hours = 2.0  # Maximum 2 hours for full processing
        self.min_listings_per_second = 1.0    # Minimum processing rate
        self.max_memory_usage_gb = 8.0        # Maximum memory usage
        
        # Espoo coordinate bounds from config
        self.espoo_bounds = {
            'min_lat': 60.1,
            'max_lat': 60.4,
            'min_lon': 24.4,
            'max_lon': 24.9
        }
        
        print(f"\nğŸ™ï¸ Testing Espoo Progressive Validation - Step 3 (Full Scale)")
        print(f"ğŸ¯ Success Criteria: â‰¥{self.required_match_rate}% match rate")
        print(f"âš¡ Performance Limits: {self.max_processing_time_hours}h, {self.min_listings_per_second} listings/s")
        print(f"ğŸ’¾ Memory Limit: {self.max_memory_usage_gb}GB")
        print(f"ğŸ”§ Max Workers: {self.max_workers}")
    
    def test_bug_prevention_prerequisites(self):
        """Bug prevention: Ensure Steps 1 and 2 passed before running Step 3"""
        print("\nğŸ”§ Bug Prevention: Prerequisites Check")
        
        # Check Step 1 and 2 results
        step1_results = list(self.output_dir.glob("espoo_step1_*.json"))
        step2_results = list(self.output_dir.glob("espoo_step2_*.json"))
        
        if step1_results and step2_results:
            print("âœ… Previous validation steps found")
        else:
            print("âš ï¸ WARNING: Previous steps not found, but continuing")
        
        # Verify system resources
        memory_gb = psutil.virtual_memory().total / (1024**3)
        cpu_count = mp.cpu_count()
        
        print(f"ğŸ’» System Resources:")
        print(f"   ğŸ’¾ Total Memory: {memory_gb:.1f}GB")
        print(f"   ğŸ”§ CPU Cores: {cpu_count}")
        
        self.assertGreater(memory_gb, 4.0, "Need at least 4GB RAM for full-scale processing")
        
        # Verify database availability
        conn = duckdb.connect(self.db_path)
        
        geocoded_espoo_count = conn.execute("""
            SELECT COUNT(*) 
            FROM listings l
            JOIN address_locations al ON l.address = al.address
            WHERE al.lat IS NOT NULL AND al.lon IS NOT NULL AND l.city = 'Espoo'
        """).fetchone()[0]
        
        conn.close()
        
        print(f"ğŸ“Š Database Status:")
        print(f"   ğŸ“ Geocoded Espoo listings: {geocoded_espoo_count:,}")
        
        self.assertGreater(geocoded_espoo_count, 50, "Need substantial geocoded dataset")
        self.total_listings_count = geocoded_espoo_count
        
        print("âœ… Prerequisites validated for full-scale processing")
    
    def test_memory_efficient_data_loading(self):
        """Test memory-efficient loading of all Espoo listings"""
        print("\nğŸ“Š Memory-Efficient Data Loading")
        
        try:
            conn = duckdb.connect(self.db_path)
            
            # Load all Espoo data efficiently
            query = """
            SELECT l.url as id, l.address, al.lat as latitude, al.lon as longitude,
                   l.price_eur as price, l.rooms, l.size_m2, l.listing_type, l.city,
                   l.title, l.postal_code, l.scraped_at
            FROM listings l
            JOIN address_locations al ON l.address = al.address
            WHERE al.lat IS NOT NULL AND al.lon IS NOT NULL AND l.city = 'Espoo'
            AND al.lat BETWEEN ? AND ? AND al.lon BETWEEN ? AND ?
            ORDER BY l.scraped_at DESC
            """
            
            df = conn.execute(query, [
                self.espoo_bounds['min_lat'], self.espoo_bounds['max_lat'],
                self.espoo_bounds['min_lon'], self.espoo_bounds['max_lon']
            ]).df()
            
            conn.close()
            
            # Validate data
            self.assertTrue((df['city'] == 'Espoo').all(), "All listings should be from Espoo")
            
            # Memory usage check
            memory_usage_mb = df.memory_usage(deep=True).sum() / (1024**2)
            print(f"ğŸ’¾ DataFrame memory usage: {memory_usage_mb:.1f}MB")
            
            # Data quality analysis
            print(f"ğŸ“Š Data Quality Analysis:")
            print(f"   ğŸ“‹ Total listings: {len(df):,}")
            print(f"   ğŸ’° Price available: {df['price'].notna().sum():,}/{len(df):,} ({df['price'].notna().sum()/len(df)*100:.1f}%)")
            
            if df['price'].notna().sum() > 0:
                print(f"   ğŸ’° Price range: â‚¬{df['price'].min():,.0f} - â‚¬{df['price'].max():,.0f}")
            
            # Store for processing
            self.all_listings = df
            return df
            
        except Exception as e:
            self.fail(f"Failed to load listings efficiently: {e}")
    
    def test_osm_buildings_optimization(self):
        """Test optimized OSM buildings loading and spatial indexing"""
        print("\nğŸ—ï¸ Optimized OSM Buildings Loading")
        
        try:
            # Load OSM buildings
            buildings_gdf = gpd.read_file(self.osm_buildings_path)
            print(f"âœ… Loaded {len(buildings_gdf):,} total OSM buildings")
            
            # Ensure proper CRS
            if buildings_gdf.crs is None or buildings_gdf.crs.to_string() != 'EPSG:4326':
                buildings_gdf = buildings_gdf.to_crs('EPSG:4326')
            
            # Filter for Espoo area with buffer
            buffer = 0.02  # ~2km buffer
            espoo_bounds_buffered = {
                'min_lat': self.espoo_bounds['min_lat'] - buffer,
                'max_lat': self.espoo_bounds['max_lat'] + buffer,
                'min_lon': self.espoo_bounds['min_lon'] - buffer,
                'max_lon': self.espoo_bounds['max_lon'] + buffer
            }
            
            # Efficient spatial filtering
            espoo_buildings = buildings_gdf[
                (buildings_gdf.geometry.bounds['miny'] >= espoo_bounds_buffered['min_lat']) &
                (buildings_gdf.geometry.bounds['maxy'] <= espoo_bounds_buffered['max_lat']) &
                (buildings_gdf.geometry.bounds['minx'] >= espoo_bounds_buffered['min_lon']) &
                (buildings_gdf.geometry.bounds['maxx'] <= espoo_bounds_buffered['max_lon'])
            ].copy()
            
            # Create spatial index
            espoo_buildings.sindex  # This creates the spatial index
            
            print(f"âœ… Filtered to {len(espoo_buildings):,} buildings in Espoo area")
            print(f"ğŸš€ Spatial index created for optimized queries")
            
            # Memory optimization
            del buildings_gdf
            gc.collect()
            
            # Store optimized buildings
            self.espoo_buildings = espoo_buildings
            self.assertGreater(len(espoo_buildings), 0, "Should have buildings in Espoo area")
            
            return espoo_buildings
            
        except Exception as e:
            self.fail(f"Failed to optimize OSM buildings loading: {e}")
    
    def test_production_scale_spatial_matching(self):
        """Test production-scale spatial matching with performance monitoring"""
        print("\nğŸ¯ Production-Scale Spatial Matching")
        
        # Ensure prerequisites
        if not hasattr(self, 'all_listings'):
            self.test_memory_efficient_data_loading()
        if not hasattr(self, 'espoo_buildings'):
            self.test_osm_buildings_optimization()
        
        df = self.all_listings
        buildings_gdf = self.espoo_buildings
        
        print(f"ğŸš€ Processing {len(df):,} listings against {len(buildings_gdf):,} buildings")
        
        # Performance monitoring setup
        start_time = time.time()
        
        # Process in chunks for memory efficiency
        all_results = []
        processed_count = 0
        
        # Split into chunks
        chunks = [df[i:i + self.chunk_size] for i in range(0, len(df), self.chunk_size)]
        total_chunks = len(chunks)
        
        print(f"ğŸ“¦ Processing {total_chunks} chunks of {self.chunk_size} listings each")
        
        # Process chunks with progress monitoring
        for chunk_idx, chunk in enumerate(chunks):
            chunk_start_time = time.time()
            
            # Process chunk with parallel workers
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # Split chunk into sub-chunks for parallel processing
                sub_chunk_size = max(1, len(chunk) // self.max_workers)
                sub_chunks = [chunk[i:i + sub_chunk_size] for i in range(0, len(chunk), sub_chunk_size)]
                
                future_to_subchunk = {
                    executor.submit(self._process_listings_chunk, sub_chunk, buildings_gdf): sub_chunk
                    for sub_chunk in sub_chunks
                }
                
                chunk_results = []
                for future in as_completed(future_to_subchunk):
                    sub_chunk_results = future.result()
                    chunk_results.extend(sub_chunk_results)
            
            all_results.extend(chunk_results)
            processed_count += len(chunk)
            
            # Progress reporting
            chunk_time = time.time() - chunk_start_time
            chunk_rate = len(chunk) / chunk_time if chunk_time > 0 else 0
            
            elapsed_time = time.time() - start_time
            overall_rate = processed_count / elapsed_time if elapsed_time > 0 else 0
            
            progress_pct = (chunk_idx + 1) / total_chunks * 100
            
            print(f"   ğŸ“Š Chunk {chunk_idx + 1}/{total_chunks} ({progress_pct:.1f}%): "
                  f"{len(chunk)} listings in {chunk_time:.1f}s "
                  f"({chunk_rate:.1f} listings/s, overall: {overall_rate:.1f} listings/s)")
            
            # Time limit check
            if elapsed_time > self.max_processing_time_hours * 3600:
                print(f"âš ï¸ Processing time limit reached: {elapsed_time/3600:.2f}h")
                break
        
        # Final performance metrics
        total_time = time.time() - start_time
        
        results_df = pd.DataFrame(all_results)
        
        # Calculate comprehensive statistics
        total_listings = len(results_df)
        matched_listings = len(results_df[results_df['matched']])
        match_rate = (matched_listings / total_listings) * 100 if total_listings > 0 else 0
        
        direct_matches = len(results_df[results_df['match_type'] == 'direct'])
        buffer_matches = len(results_df[results_df['match_type'] == 'buffer'])
        no_matches = total_listings - matched_listings
        
        listings_per_second = total_listings / total_time if total_time > 0 else 0
        
        print(f"\nğŸ“Š Production-Scale Matching Results:")
        print(f"   ğŸ“‹ Total listings processed: {total_listings:,}")
        print(f"   âœ… Overall match rate: {match_rate:.3f}%")
        print(f"   ğŸ¯ Direct matches: {direct_matches:,} ({direct_matches/total_listings*100:.2f}%)")
        print(f"   ğŸ” Buffer matches: {buffer_matches:,} ({buffer_matches/total_listings*100:.2f}%)")
        print(f"   âŒ No matches: {no_matches:,} ({no_matches/total_listings*100:.2f}%)")
        print(f"   â±ï¸ Total processing time: {total_time:.1f}s ({total_time/60:.1f}min)")
        print(f"   ğŸš€ Processing rate: {listings_per_second:.2f} listings/second")
        
        # Performance validation
        self.assertLess(total_time, self.max_processing_time_hours * 3600,
                       f"Processing should complete within {self.max_processing_time_hours} hours")
        self.assertGreater(listings_per_second, self.min_listings_per_second,
                          f"Should process at least {self.min_listings_per_second} listings per second")
        
        # Quality validation
        self.assertGreaterEqual(match_rate, self.required_match_rate,
                               f"Match rate {match_rate:.3f}% should be â‰¥ {self.required_match_rate}%")
        
        print(f"âœ… SUCCESS: Match rate {match_rate:.3f}% meets production requirement â‰¥ {self.required_match_rate}%")
        print(f"âœ… Performance benchmarks met: {listings_per_second:.2f} listings/s")
        
        # Store results
        self.validation_results = results_df
        self.match_rate = match_rate
        self.performance_metrics = {
            'total_time_seconds': total_time,
            'listings_per_second': listings_per_second,
            'chunks_processed': len(chunks),
            'max_workers': self.max_workers
        }
        
        return results_df, match_rate
    
    def _process_listings_chunk(self, chunk_df, buildings_gdf):
        """Process a chunk of listings for spatial matching (optimized version)"""
        chunk_results = []
        
        for idx, listing in chunk_df.iterrows():
            # Create point geometry
            point = Point(listing['longitude'], listing['latitude'])
            
            # Step 1: Direct containment check using spatial index
            with warnings.catch_warnings():
                warnings.simplefilter("ignore", UserWarning)
                possible_matches_idx = list(buildings_gdf.sindex.intersection(point.bounds))
                possible_matches = buildings_gdf.iloc[possible_matches_idx]
                containing_buildings = possible_matches[possible_matches.contains(point)]
            
            if not containing_buildings.empty:
                # Direct match found
                building = containing_buildings.iloc[0]
                building_name = building.get('name', '')
                
                chunk_results.append({
                    'listing_id': listing['id'],
                    'address': listing['address'],
                    'latitude': listing['latitude'],
                    'longitude': listing['longitude'],
                    'price': listing['price'],
                    'match_type': 'direct',
                    'building_id': building.get('osm_id', 'N/A'),
                    'building_name': building_name,
                    'distance_m': 0.0,
                    'matched': True,
                    'quality_score': 1.0
                })
            else:
                # Step 2: Buffer search using spatial index
                buffer_distance = 0.001  # ~100m
                buffered_point = point.buffer(buffer_distance)
                
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore", UserWarning)
                    buffer_matches_idx = list(buildings_gdf.sindex.intersection(buffered_point.bounds))
                    buffer_matches = buildings_gdf.iloc[buffer_matches_idx]
                    intersecting_buildings = buffer_matches[buffer_matches.intersects(buffered_point)]
                
                if not intersecting_buildings.empty:
                    # Find closest building
                    with warnings.catch_warnings():
                        warnings.simplefilter("ignore", UserWarning)
                        distances = intersecting_buildings.geometry.distance(point)
                    
                    closest_idx = distances.idxmin()
                    closest_building = intersecting_buildings.loc[closest_idx]
                    closest_distance = distances.loc[closest_idx] * 111000  # Convert to meters
                    
                    building_name = closest_building.get('name', '')
                    
                    chunk_results.append({
                        'listing_id': listing['id'],
                        'address': listing['address'],
                        'latitude': listing['latitude'],
                        'longitude': listing['longitude'],
                        'price': listing['price'],
                        'match_type': 'buffer',
                        'building_id': closest_building.get('osm_id', 'N/A'),
                        'building_name': building_name,
                        'distance_m': closest_distance,
                        'matched': True,
                        'quality_score': max(0.1, 1.0 - (closest_distance / 100.0))
                    })
                else:
                    # No match
                    chunk_results.append({
                        'listing_id': listing['id'],
                        'address': listing['address'],
                        'latitude': listing['latitude'],
                        'longitude': listing['longitude'],
                        'price': listing['price'],
                        'match_type': 'none',
                        'building_id': None,
                        'building_name': None,
                        'distance_m': float('inf'),
                        'matched': False,
                        'quality_score': 0.0
                    })
        
        return chunk_results
    
    def test_comprehensive_production_metrics(self):
        """Test comprehensive production-ready metrics"""
        print("\nğŸ“Š Comprehensive Production Metrics")
        
        # Ensure we have validation results
        if not hasattr(self, 'validation_results'):
            self.test_production_scale_spatial_matching()
        
        results_df = self.validation_results
        
        # Calculate comprehensive production metrics
        metrics = {
            'timestamp': datetime.now().isoformat(),
            'validation_type': 'espoo_step3_full_scale_production',
            'city': self.city,
            'sample_size': len(results_df),
            'production_readiness': {
                'match_rate_percent': self.match_rate,
                'required_match_rate': self.required_match_rate,
                'meets_production_criteria': self.match_rate >= self.required_match_rate,
                'quality_grade': self._calculate_quality_grade(self.match_rate)
            },
            'match_statistics': {
                'total_matched': len(results_df[results_df['matched']]),
                'match_rate_percent': self.match_rate,
                'direct_matches': len(results_df[results_df['match_type'] == 'direct']),
                'buffer_matches': len(results_df[results_df['match_type'] == 'buffer']),
                'no_matches': len(results_df[results_df['match_type'] == 'none']),
                'avg_quality_score': float(results_df['quality_score'].mean())
            },
            'performance_metrics': self.performance_metrics,
            'success_criteria': {
                'required_match_rate': self.required_match_rate,
                'achieved_match_rate': self.match_rate,
                'meets_criteria': self.match_rate >= self.required_match_rate
            }
        }
        
        # Save metrics to file
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        metrics_path = self.output_dir / f"espoo_step3_production_metrics_{timestamp}.json"
        
        with open(metrics_path, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Production metrics saved: {metrics_path}")
        print(f"ğŸ“Š Quality Grade: {metrics['production_readiness']['quality_grade']}")
        print(f"âš¡ Processing Efficiency: {metrics['performance_metrics']['listings_per_second']:.2f} listings/s")
        
        # Store for reporting
        self.production_metrics = metrics
        return metrics
    
    def _calculate_quality_grade(self, match_rate):
        """Calculate quality grade based on match rate"""
        if match_rate >= 99.5:
            return "A+"
        elif match_rate >= 99.0:
            return "A"
        elif match_rate >= 98.0:
            return "B+"
        elif match_rate >= 95.0:
            return "B"
        elif match_rate >= 90.0:
            return "C"
        else:
            return "D"
    
    def test_production_validation_report(self):
        """Test generation of production validation report"""
        print("\nğŸ“„ Generating Production Validation Report")
        
        # Ensure we have all required data
        if not hasattr(self, 'production_metrics'):
            self.test_comprehensive_production_metrics()
        
        metrics = self.production_metrics
        
        # Generate comprehensive HTML report
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = self.output_dir / f"espoo_step3_production_report_{timestamp}.html"
        
        html_content = self._generate_production_html_report(metrics)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        self.assertTrue(report_path.exists(), "Production validation report should be created")
        print(f"âœ… Production validation report generated: {report_path}")
        
        return report_path
    
    def _generate_production_html_report(self, metrics):
        """Generate comprehensive production HTML report"""
        success_class = "success" if metrics['production_readiness']['meets_production_criteria'] else "error"
        
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Espoo Progressive Validation - Step 3: Production Scale</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .success {{ color: green; font-weight: bold; }}
                .error {{ color: red; font-weight: bold; }}
                .metric-box {{ 
                    border: 1px solid #ddd; 
                    padding: 15px; 
                    margin: 10px 0; 
                    border-radius: 5px; 
                    background-color: #f9f9f9; 
                }}
                .production {{ background-color: #e8f5e8; }}
            </style>
        </head>
        <body>
            <h1>ğŸ™ï¸ Espoo Progressive Validation - Step 3: Production Scale</h1>
            
            <div class="metric-box production">
                <h2>ğŸš€ Production Readiness Assessment</h2>
                <p><strong>Overall Result:</strong> 
                   <span class="{success_class}">
                   {'âœ… PRODUCTION READY' if metrics['production_readiness']['meets_production_criteria'] else 'âŒ NOT READY'}
                   </span>
                </p>
                <p><strong>Match Rate:</strong> 
                   <span class="{success_class}">{metrics['production_readiness']['match_rate_percent']:.3f}%</span>
                </p>
                <p><strong>Required Rate:</strong> {metrics['production_readiness']['required_match_rate']:.2f}%</p>
                <p><strong>Quality Grade:</strong> {metrics['production_readiness']['quality_grade']}</p>
                <p><strong>Sample Size:</strong> {metrics['sample_size']:,} listings</p>
                <p><strong>Validation Date:</strong> {metrics['timestamp']}</p>
            </div>
            
            <div class="metric-box">
                <h2>ğŸ¯ Match Statistics</h2>
                <p><strong>Total Matched:</strong> {metrics['match_statistics']['total_matched']:,}</p>
                <p><strong>Direct Matches:</strong> {metrics['match_statistics']['direct_matches']:,}</p>
                <p><strong>Buffer Matches:</strong> {metrics['match_statistics']['buffer_matches']:,}</p>
                <p><strong>No Matches:</strong> {metrics['match_statistics']['no_matches']:,}</p>
                <p><strong>Average Quality Score:</strong> {metrics['match_statistics']['avg_quality_score']:.3f}</p>
            </div>
            
            <div class="metric-box">
                <h2>âš¡ Performance Metrics</h2>
                <p><strong>Processing Time:</strong> {metrics['performance_metrics']['total_time_seconds']:.1f} seconds</p>
                <p><strong>Processing Rate:</strong> {metrics['performance_metrics']['listings_per_second']:.2f} listings/second</p>
                <p><strong>Chunks Processed:</strong> {metrics['performance_metrics']['chunks_processed']}</p>
                <p><strong>Workers Used:</strong> {metrics['performance_metrics']['max_workers']}</p>
            </div>
            
            <div class="metric-box">
                <h2>ğŸ‰ Completion Status</h2>
                <p>âœ… <strong>Step 1 Complete:</strong> 10 Sample Validation</p>
                <p>âœ… <strong>Step 2 Complete:</strong> 100 Sample Validation</p>
                <p>âœ… <strong>Step 3 Complete:</strong> Full-Scale Production Validation</p>
                <p>ğŸš€ <strong>Next:</strong> Deploy to Production Environment</p>
                <p>ğŸ“ <strong>Recommendation:</strong> {'Proceed with production deployment' if metrics['production_readiness']['meets_production_criteria'] else 'Address quality issues before production'}</p>
            </div>
            
        </body>
        </html>
        """
        
        return html_content
    
    def test_complete_production_workflow(self):
        """Test the complete Step 3 production validation workflow"""
        print("\nğŸš€ Testing Complete Step 3 Production Validation Workflow")
        
        # Run all validation steps
        self.test_bug_prevention_prerequisites()
        df = self.test_memory_efficient_data_loading()
        buildings_gdf = self.test_osm_buildings_optimization()
        results_df, match_rate = self.test_production_scale_spatial_matching()
        production_metrics = self.test_comprehensive_production_metrics()
        report_path = self.test_production_validation_report()
        
        # Final validation
        success = match_rate >= self.required_match_rate
        
        print(f"\nğŸ“Š STEP 3 PRODUCTION VALIDATION SUMMARY")
        print("=" * 80)
        print(f"ğŸ™ï¸ City: {self.city}")
        print(f"ğŸ“‹ Total Listings: {len(results_df):,}")
        print(f"ğŸ“ˆ Match Rate: {match_rate:.3f}%")
        print(f"ğŸ¯ Production Criteria: â‰¥{self.required_match_rate}%")
        print(f"ğŸ’ Quality Grade: {production_metrics['production_readiness']['quality_grade']}")
        print(f"âš¡ Processing Rate: {production_metrics['performance_metrics']['listings_per_second']:.2f} listings/s")
        print(f"âœ… Result: {'PRODUCTION READY' if success else 'NOT READY FOR PRODUCTION'}")
        print(f"ğŸ“„ Report: {report_path}")
        
        if success:
            print(f"\nğŸ‰ ESPOO EXPANSION VALIDATION COMPLETE!")
            print(f"   âœ… All 3 validation steps passed")
            print(f"   ğŸš€ Ready for production deployment")
            print(f"   ğŸ“Š Quality Grade: {production_metrics['production_readiness']['quality_grade']}")
            print(f"   ğŸ“„ Full report: {report_path}")
        else:
            print(f"\nâŒ Step 3 Failed - Production Readiness Issues:")
            print(f"   ğŸ“‰ Match rate {match_rate:.3f}% below required {self.required_match_rate}%")
            print(f"   ğŸ”§ Review spatial matching algorithms")
        
        self.assertTrue(success, f"Step 3 validation should succeed with â‰¥{self.required_match_rate}% match rate")


def run_espoo_step3_validation():
    """Run the Espoo Step 3 full-scale validation test suite"""
    print("ğŸ™ï¸ Espoo Progressive Validation: Step 3 - Full Scale Production Test")
    print("=" * 90)
    print("Testing full-scale Espoo production readiness with comprehensive benchmarks")
    print("Success Criteria: â‰¥99.40% match rate for production deployment")
    print("Requirements: 5.1, 5.2, 5.3, 5.4")
    print("=" * 90)
    
    # Create test suite
    suite = unittest.TestLoader().loadTestsFromTestCase(TestEspooStep3FullScale)
    
    # Run tests
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # Return success/failure
    return result.wasSuccessful()


if __name__ == "__main__":
    success = run_espoo_step3_validation()
    sys.exit(0 if success else 1)